{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Modelo de Predicción de Gasto Semanal con LSTM\n",
    "\n",
    "Este notebook implementa un modelo de predicción de gasto semanal utilizando redes neuronales recurrentes LSTM (Long Short-Term Memory).\n",
    "La predicción se realiza con periodos de 1 semana, como se especificó en los requerimientos.\n"
   ],
   "id": "460dd4f051e27a2c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T22:22:58.170418Z",
     "start_time": "2025-05-24T22:22:58.134455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 1. IMPORTAR LIBRERÍAS --------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import holidays\n",
    "\n",
    "# Librerías para deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "from fastapi import FastAPI\n",
    "\n",
    "# Configurar semilla para reproducibilidad\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n"
   ],
   "id": "23569af1579f85b8",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T22:22:58.999943Z",
     "start_time": "2025-05-24T22:22:58.183025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 2. CARGAR Y PREPROCESAR DATOS --------------------------------------\n",
    "# Cargar datos de clientes\n",
    "clients = pd.read_csv('../data/raw/base_clientes_final.csv')\n",
    "clients.info()\n",
    "\n",
    "# Cargar datos de transacciones\n",
    "txn = pd.read_csv('../data/raw/base_transacciones_final.csv')\n",
    "txn.info()\n",
    "\n",
    "# Convertir fecha a datetime y crear columna de semana\n",
    "txn[\"fecha\"] = pd.to_datetime(txn[\"fecha\"])\n",
    "txn[\"week\"] = txn[\"fecha\"].dt.to_period(\"W\")\n",
    "\n",
    "# Agregar datos por cliente y semana\n",
    "agg = (txn.groupby([\"id\", \"week\"])\n",
    "           .agg(spend=(\"monto\", \"sum\"),\n",
    "                n_tx=(\"monto\", \"size\"),\n",
    "                max_tx=(\"monto\", \"max\"),\n",
    "                avg_ticket=(\"monto\", \"mean\"))\n",
    "           .reset_index())\n"
   ],
   "id": "63859156f26704aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 8 columns):\n",
      " #   Column                 Non-Null Count  Dtype \n",
      "---  ------                 --------------  ----- \n",
      " 0   id                     1000 non-null   object\n",
      " 1   fecha_nacimiento       1000 non-null   object\n",
      " 2   fecha_alta             1000 non-null   object\n",
      " 3   id_municipio           1000 non-null   int64 \n",
      " 4   id_estado              1000 non-null   int64 \n",
      " 5   tipo_persona           1000 non-null   object\n",
      " 6   genero                 1000 non-null   object\n",
      " 7   actividad_empresarial  1000 non-null   object\n",
      "dtypes: int64(2), object(6)\n",
      "memory usage: 62.6+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 346011 entries, 0 to 346010\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count   Dtype  \n",
      "---  ------         --------------   -----  \n",
      " 0   id             346011 non-null  object \n",
      " 1   fecha          346011 non-null  object \n",
      " 2   comercio       346011 non-null  object \n",
      " 3   giro_comercio  340423 non-null  object \n",
      " 4   tipo_venta     346011 non-null  object \n",
      " 5   monto          346011 non-null  float64\n",
      "dtypes: float64(1), object(5)\n",
      "memory usage: 15.8+ MB\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T22:22:59.185889Z",
     "start_time": "2025-05-24T22:22:59.123621Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 3. COMPLETAR CALENDARIO --------------------------------------------\n",
    "# Crear un índice completo para todas las semanas y clientes\n",
    "full_idx = pd.MultiIndex.from_product(\n",
    "    [agg[\"id\"].unique(),\n",
    "     pd.period_range(agg[\"week\"].min(), agg[\"week\"].max(), freq=\"W\")],\n",
    "    names=[\"id\", \"week\"]\n",
    ")\n",
    "panel = (agg.set_index([\"id\", \"week\"])\n",
    "             .reindex(full_idx, fill_value=0)\n",
    "             .reset_index())\n"
   ],
   "id": "b3f372e9a0f4178c",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T22:22:59.250607Z",
     "start_time": "2025-05-24T22:22:59.215857Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 4. DATOS ESTÁTICOS DE CLIENTES -------------------------------------------\n",
    "# Fecha fija para reproducibilidad\n",
    "today = pd.Timestamp(\"2025-05-24\")\n",
    "# Calcular edad y antigüedad en meses\n",
    "clients[\"age\"] = ((today - pd.to_datetime(clients[\"fecha_nacimiento\"])).dt.days // 365)\n",
    "clients[\"tenure_months\"] = ((today - pd.to_datetime(clients[\"fecha_alta\"])).dt.days // 30)\n",
    "\n",
    "# Seleccionar columnas estáticas y unir con panel\n",
    "static_cols = [\"id\", \"age\", \"tenure_months\", \"id_estado\",\n",
    "               \"tipo_persona\", \"genero\", \"actividad_empresarial\"]\n",
    "panel = panel.merge(clients[static_cols], on=\"id\", how=\"left\")\n"
   ],
   "id": "ac7294797a07b7b6",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T22:22:59.848609Z",
     "start_time": "2025-05-24T22:22:59.275106Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 5. CARACTERÍSTICAS DE REZAGO (LAG FEATURES) -------------------------------------------------\n",
    "panel = panel.sort_values([\"id\", \"week\"])\n",
    "# Crear variables de rezago para gasto y otras métricas\n",
    "for k in range(1, 7):\n",
    "    panel[f\"spend_lag{k}\"] = panel.groupby(\"id\")[\"spend\"].shift(k)\n",
    "\n",
    "# También crear rezagos para número de transacciones y ticket promedio\n",
    "for k in range(1, 4):\n",
    "    panel[f\"n_tx_lag{k}\"] = panel.groupby(\"id\")[\"n_tx\"].shift(k)\n",
    "    panel[f\"avg_ticket_lag{k}\"] = panel.groupby(\"id\")[\"avg_ticket\"].shift(k)\n",
    "\n",
    "# Crear variables de media móvil y estadísticas\n",
    "panel[\"rolling_mean_3\"] = panel.groupby(\"id\")[\"spend\"].rolling(3).mean().reset_index(level=0, drop=True)\n",
    "panel[\"rolling_mean_6\"] = panel.groupby(\"id\")[\"spend\"].rolling(6).mean().reset_index(level=0, drop=True)\n",
    "panel[\"rolling_std_3\"] = panel.groupby(\"id\")[\"spend\"].rolling(3).std().reset_index(level=0, drop=True)\n",
    "panel[\"rolling_std_6\"] = panel.groupby(\"id\")[\"spend\"].rolling(6).std().reset_index(level=0, drop=True)\n",
    "panel[\"rolling_max_3\"] = panel.groupby(\"id\")[\"spend\"].rolling(3).max().reset_index(level=0, drop=True)\n",
    "panel[\"rolling_min_3\"] = panel.groupby(\"id\")[\"spend\"].rolling(3).min().reset_index(level=0, drop=True)\n",
    "\n",
    "# Calcular tendencias y tasas de cambio\n",
    "panel[\"spend_trend\"] = panel[\"spend\"] - panel[\"rolling_mean_6\"]\n",
    "panel[\"spend_pct_change\"] = panel.groupby(\"id\")[\"spend\"].pct_change()\n",
    "# Reemplazar infinitos con NaN en pct_change\n",
    "panel[\"spend_pct_change\"].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "panel[\"spend_acceleration\"] = panel.groupby(\"id\")[\"spend_pct_change\"].diff()\n",
    "# Reemplazar infinitos con NaN en acceleration\n",
    "panel[\"spend_acceleration\"].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "# Volatilidad relativa (coeficiente de variación)\n",
    "panel[\"spend_volatility\"] = panel[\"rolling_std_6\"] / (panel[\"rolling_mean_6\"] + 1)  # +1 para evitar división por cero\n"
   ],
   "id": "52743203363d6ff9",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T22:23:08.324445Z",
     "start_time": "2025-05-24T22:22:59.930363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 6. ESTACIONALIDAD Y DÍAS FESTIVOS --------------------------------------\n",
    "# Características cíclicas para la semana del año\n",
    "panel[\"week_of_year\"] = panel[\"week\"].dt.week\n",
    "panel[\"week_sin\"] = np.sin(2 * np.pi * panel[\"week_of_year\"] / 52)\n",
    "panel[\"week_cos\"] = np.cos(2 * np.pi * panel[\"week_of_year\"] / 52)\n",
    "\n",
    "# También mantener información del mes para estacionalidad mensual\n",
    "panel[\"month\"] = panel[\"week\"].dt.asfreq('M')\n",
    "panel[\"month_idx\"] = panel[\"month\"].dt.month\n",
    "panel[\"month_sin\"] = np.sin(2 * np.pi * panel[\"month_idx\"] / 12)\n",
    "panel[\"month_cos\"] = np.cos(2 * np.pi * panel[\"month_idx\"] / 12)\n",
    "\n",
    "# Agregar fechas importantes en México\n",
    "def is_buen_fin(date): return date.month == 11 and date.day >= 15\n",
    "def is_navidad(date): return date.month == 12 and date.day >= 20\n",
    "def is_mothers_day(date): return date.month == 5 and date.day == 10\n",
    "\n",
    "txn[\"buen_fin\"] = txn[\"fecha\"].apply(is_buen_fin)\n",
    "txn[\"navidad\"] = txn[\"fecha\"].apply(is_navidad)\n",
    "txn[\"dia_madre\"] = txn[\"fecha\"].apply(is_mothers_day)\n",
    "\n",
    "# Agregar variables de días festivos al panel\n",
    "holiday_agg = txn.groupby([\"id\", txn[\"fecha\"].dt.to_period(\"W\")])[[\"buen_fin\", \"navidad\", \"dia_madre\"]].sum().reset_index()\n",
    "holiday_agg.rename(columns={\"fecha\": \"week\"}, inplace=True)\n",
    "panel = panel.merge(holiday_agg, on=[\"id\", \"week\"], how=\"left\").fillna(0)\n",
    "\n",
    "# Agregar días festivos de México usando el paquete holidays\n",
    "def add_holiday_feature(df):\n",
    "    \"\"\"Agrega una columna is_holiday al DataFrame\"\"\"\n",
    "    # Convertir week a datetime para poder comparar con holidays\n",
    "    df['week_start'] = df['week'].dt.start_time\n",
    "\n",
    "    # Obtener días festivos de México para los años relevantes\n",
    "    start_year = df['week_start'].dt.year.min()\n",
    "    end_year = df['week_start'].dt.year.max()\n",
    "    mx_holidays = holidays.Mexico(years=range(start_year, end_year + 1))\n",
    "\n",
    "    # Marcar semanas que contienen días festivos\n",
    "    df['is_holiday'] = df['week_start'].apply(\n",
    "        lambda x: any(x <= pd.Timestamp(day) <= x + pd.Timedelta(days=6) for day in mx_holidays if start_year <= day.year <= end_year)\n",
    "    ).astype(int)\n",
    "\n",
    "    # Eliminar columna temporal\n",
    "    df.drop('week_start', axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "panel = add_holiday_feature(panel)\n",
    "\n",
    "# Agregar indicador de fin de semana (aunque para datos semanales no es tan relevante)\n",
    "panel['is_weekend'] = 1  # Todas las semanas tienen fin de semana\n"
   ],
   "id": "bba81ab6d7b7ee4a",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T22:23:08.450633Z",
     "start_time": "2025-05-24T22:23:08.394475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 7. VARIABLE OBJETIVO Y PREPROCESAMIENTO FINAL -------------------------------------------------------\n",
    "# La variable objetivo es el gasto de la próxima semana\n",
    "panel[\"spend_next\"] = panel.groupby(\"id\")[\"spend\"].shift(-1)\n",
    "\n",
    "# Crear características de interacción\n",
    "panel[\"spend_by_age\"] = panel[\"spend\"] / (panel[\"age\"] + 1)  # +1 para evitar división por cero\n",
    "panel[\"spend_by_tenure\"] = panel[\"spend\"] / (panel[\"tenure_months\"] + 1)\n",
    "panel[\"tx_frequency\"] = panel[\"n_tx\"] / (panel[\"tenure_months\"] + 1)\n",
    "panel[\"spend_per_tx\"] = panel[\"spend\"] / (panel[\"n_tx\"] + 1)\n",
    "\n",
    "# Características no lineales\n",
    "panel[\"spend_squared\"] = panel[\"spend\"] ** 2\n",
    "panel[\"log_spend\"] = np.log1p(panel[\"spend\"])  # log(1+x) para manejar ceros\n",
    "\n",
    "# Relación entre gasto actual y promedio histórico\n",
    "panel[\"spend_vs_history\"] = panel[\"spend\"] / (panel[\"rolling_mean_6\"] + 1)\n",
    "\n",
    "# Manejo de valores faltantes para características numéricas\n",
    "numeric_cols = panel.select_dtypes(include=['float64', 'int64']).columns\n",
    "for col in numeric_cols:\n",
    "    if col != \"spend_next\" and panel[col].isnull().sum() > 0:\n",
    "        # Imputar con la mediana por cliente, o la mediana global si no hay datos del cliente\n",
    "        panel[col] = panel.groupby(\"id\")[col].transform(\n",
    "            lambda x: x.fillna(x.median() if not pd.isna(x.median()) else panel[col].median())\n",
    "        )\n",
    "\n",
    "# Eliminar filas sin valores para la variable objetivo después de la imputación\n",
    "panel = panel.dropna(subset=[\"spend_next\"])\n"
   ],
   "id": "789ddf885869a2d1",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T22:23:18.022344Z",
     "start_time": "2025-05-24T22:23:08.465811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 8. PREPARACIÓN DE DATOS PARA LSTM --------------------------------------------------------\n",
    "# Definir columnas de características\n",
    "feature_cols = [\n",
    "    # Características básicas de transacciones\n",
    "    \"spend\", \"n_tx\", \"max_tx\", \"avg_ticket\",\n",
    "\n",
    "    # Características de rezago para gasto\n",
    "    \"spend_lag1\", \"spend_lag2\", \"spend_lag3\", \"spend_lag4\", \"spend_lag5\", \"spend_lag6\",\n",
    "\n",
    "    # Características de rezago para otras métricas\n",
    "    \"n_tx_lag1\", \"n_tx_lag2\", \"n_tx_lag3\",\n",
    "    \"avg_ticket_lag1\", \"avg_ticket_lag2\", \"avg_ticket_lag3\",\n",
    "\n",
    "    # Estadísticas móviles\n",
    "    \"rolling_mean_3\", \"rolling_mean_6\", \n",
    "    \"rolling_std_3\", \"rolling_std_6\",\n",
    "    \"rolling_max_3\", \"rolling_min_3\",\n",
    "\n",
    "    # Tendencias y cambios\n",
    "    \"spend_trend\", \"spend_pct_change\", \"spend_acceleration\", \"spend_volatility\",\n",
    "\n",
    "    # Características de interacción\n",
    "    \"spend_by_age\", \"spend_by_tenure\", \"tx_frequency\", \"spend_per_tx\",\n",
    "    \"spend_vs_history\",\n",
    "\n",
    "    # Transformaciones no lineales\n",
    "    \"spend_squared\", \"log_spend\",\n",
    "\n",
    "    # Características demográficas\n",
    "    \"age\", \"tenure_months\",\n",
    "\n",
    "    # Características estacionales\n",
    "    \"week_sin\", \"week_cos\", \"month_sin\", \"month_cos\",\n",
    "    \"buen_fin\", \"navidad\", \"dia_madre\", \"is_holiday\", \"is_weekend\"\n",
    "]\n",
    "\n",
    "# Convertir variables categóricas a one-hot encoding\n",
    "cat_features = [\"id_estado\", \"tipo_persona\", \"genero\", \"actividad_empresarial\"]\n",
    "panel_encoded = pd.get_dummies(panel, columns=cat_features, drop_first=True)\n",
    "\n",
    "# Actualizar lista de características con columnas one-hot\n",
    "for col in panel_encoded.columns:\n",
    "    if any(col.startswith(f\"{cat}_\") for cat in cat_features):\n",
    "        feature_cols.append(col)\n",
    "\n",
    "# División cronológica (mismo corte para todos los clientes)\n",
    "# Para datos semanales, usamos las últimas 12 semanas como validación y test\n",
    "train_mask = panel_encoded[\"week\"] <= panel_encoded[\"week\"].max() - 12   # todas las semanas excepto las últimas 12\n",
    "val_mask = (panel_encoded[\"week\"] > panel_encoded[\"week\"].max() - 12) & (panel_encoded[\"week\"] <= panel_encoded[\"week\"].max() - 4)  # 8 semanas para validación\n",
    "test_mask = panel_encoded[\"week\"] > panel_encoded[\"week\"].max() - 4      # últimas 4 semanas para test\n",
    "\n",
    "# Función para crear secuencias para LSTM\n",
    "def create_sequences(data, client_ids, seq_length=6):\n",
    "    \"\"\"\n",
    "    Crea secuencias de datos para LSTM agrupadas por cliente.\n",
    "\n",
    "    Args:\n",
    "        data: DataFrame con los datos\n",
    "        client_ids: Lista de IDs de clientes a incluir\n",
    "        seq_length: Longitud de la secuencia (número de semanas anteriores)\n",
    "\n",
    "    Returns:\n",
    "        X: Secuencias de entrada (samples, time steps, features)\n",
    "        y: Valores objetivo\n",
    "        client_map: Mapeo de índices a IDs de clientes\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    client_map = []\n",
    "\n",
    "    for client_id in client_ids:\n",
    "        # Filtrar datos del cliente\n",
    "        client_data = data[data['id'] == client_id].sort_values('week')\n",
    "\n",
    "        if len(client_data) <= seq_length:\n",
    "            continue\n",
    "\n",
    "        # Extraer características y objetivo\n",
    "        features = client_data[feature_cols].values\n",
    "        target = client_data['spend_next'].values\n",
    "\n",
    "        # Crear secuencias\n",
    "        for i in range(len(features) - seq_length):\n",
    "            X.append(features[i:i+seq_length])\n",
    "            y.append(target[i+seq_length-1])\n",
    "            client_map.append(client_id)\n",
    "\n",
    "    return np.array(X), np.array(y), client_map\n",
    "\n",
    "# Obtener listas de clientes para cada conjunto\n",
    "train_clients = panel_encoded.loc[train_mask, 'id'].unique()\n",
    "val_clients = panel_encoded.loc[val_mask, 'id'].unique()\n",
    "test_clients = panel_encoded.loc[test_mask, 'id'].unique()\n",
    "\n",
    "# Crear secuencias para entrenamiento, validación y prueba\n",
    "X_train, y_train, train_client_map = create_sequences(panel_encoded, train_clients)\n",
    "X_val, y_val, val_client_map = create_sequences(panel_encoded, val_clients)\n",
    "X_test, y_test, test_client_map = create_sequences(panel_encoded, test_clients)\n",
    "\n",
    "print(f\"Forma de X_train: {X_train.shape}\")\n",
    "print(f\"Forma de X_val: {X_val.shape}\")\n",
    "print(f\"Forma de X_test: {X_test.shape}\")\n"
   ],
   "id": "afc1043b75c6093c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma de X_train: (51000, 6, 106)\n",
      "Forma de X_val: (51000, 6, 106)\n",
      "Forma de X_test: (51000, 6, 106)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-24T22:23:19.766236Z",
     "start_time": "2025-05-24T22:23:18.179579Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 9. NORMALIZACIÓN DE DATOS --------------------------------------\n",
    "# Normalizar características para mejorar el entrenamiento del LSTM\n",
    "# Reshape para aplicar el scaler\n",
    "n_samples_train, n_timesteps, n_features = X_train.shape\n",
    "X_train_reshaped = X_train.reshape(n_samples_train * n_timesteps, n_features)\n",
    "\n",
    "# Función para limpiar datos antes de escalar\n",
    "def clean_data_for_scaling(data):\n",
    "    # Reemplazar infinitos y NaN con valores finitos\n",
    "    data_clean = np.nan_to_num(data, nan=0.0, posinf=1e15, neginf=-1e15)\n",
    "    # Recortar valores extremos\n",
    "    data_clean = np.clip(data_clean, -1e15, 1e15)\n",
    "    return data_clean\n",
    "\n",
    "# Limpiar y ajustar el scaler solo en los datos de entrenamiento\n",
    "X_train_reshaped_clean = clean_data_for_scaling(X_train_reshaped)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_reshaped_clean)\n",
    "\n",
    "# Volver a la forma original\n",
    "X_train_scaled = X_train_scaled.reshape(n_samples_train, n_timesteps, n_features)\n",
    "\n",
    "# Aplicar la misma transformación a los datos de validación y prueba\n",
    "n_samples_val, _, _ = X_val.shape\n",
    "X_val_reshaped = X_val.reshape(n_samples_val * n_timesteps, n_features)\n",
    "# Limpiar datos de validación antes de transformar\n",
    "X_val_reshaped_clean = clean_data_for_scaling(X_val_reshaped)\n",
    "X_val_scaled = scaler.transform(X_val_reshaped_clean)\n",
    "X_val_scaled = X_val_scaled.reshape(n_samples_val, n_timesteps, n_features)\n",
    "\n",
    "n_samples_test, _, _ = X_test.shape\n",
    "X_test_reshaped = X_test.reshape(n_samples_test * n_timesteps, n_features)\n",
    "# Limpiar datos de prueba antes de transformar\n",
    "X_test_reshaped_clean = clean_data_for_scaling(X_test_reshaped)\n",
    "X_test_scaled = scaler.transform(X_test_reshaped_clean)\n",
    "X_test_scaled = X_test_scaled.reshape(n_samples_test, n_timesteps, n_features)\n",
    "\n",
    "# Normalizar también la variable objetivo\n",
    "y_scaler = StandardScaler()\n",
    "# Limpiar datos de la variable objetivo antes de escalar\n",
    "y_train_clean = clean_data_for_scaling(y_train.reshape(-1, 1))\n",
    "y_train_scaled = y_scaler.fit_transform(y_train_clean).flatten()\n",
    "\n",
    "# Aplicar la misma transformación a los datos de validación y prueba\n",
    "y_val_clean = clean_data_for_scaling(y_val.reshape(-1, 1))\n",
    "y_val_scaled = y_scaler.transform(y_val_clean).flatten()\n",
    "\n",
    "y_test_clean = clean_data_for_scaling(y_test.reshape(-1, 1))\n",
    "y_test_scaled = y_scaler.transform(y_test_clean).flatten()\n"
   ],
   "id": "891b328ee326ba1f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/facundobautistabarbera/PycharmProjects/Analisis-Exploratorio/.venv/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[15]\u001B[39m\u001B[32m, line 9\u001B[39m\n\u001B[32m      7\u001B[39m \u001B[38;5;66;03m# Ajustar el scaler solo en los datos de entrenamiento\u001B[39;00m\n\u001B[32m      8\u001B[39m scaler = StandardScaler()\n\u001B[32m----> \u001B[39m\u001B[32m9\u001B[39m X_train_scaled = \u001B[43mscaler\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX_train_reshaped\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[38;5;66;03m# Volver a la forma original\u001B[39;00m\n\u001B[32m     12\u001B[39m X_train_scaled = X_train_scaled.reshape(n_samples_train, n_timesteps, n_features)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Analisis-Exploratorio/.venv/lib/python3.12/site-packages/sklearn/utils/_set_output.py:319\u001B[39m, in \u001B[36m_wrap_method_output.<locals>.wrapped\u001B[39m\u001B[34m(self, X, *args, **kwargs)\u001B[39m\n\u001B[32m    317\u001B[39m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[32m    318\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, *args, **kwargs):\n\u001B[32m--> \u001B[39m\u001B[32m319\u001B[39m     data_to_wrap = \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    320\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[32m    321\u001B[39m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[32m    322\u001B[39m         return_tuple = (\n\u001B[32m    323\u001B[39m             _wrap_data_with_container(method, data_to_wrap[\u001B[32m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[32m    324\u001B[39m             *data_to_wrap[\u001B[32m1\u001B[39m:],\n\u001B[32m    325\u001B[39m         )\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Analisis-Exploratorio/.venv/lib/python3.12/site-packages/sklearn/base.py:918\u001B[39m, in \u001B[36mTransformerMixin.fit_transform\u001B[39m\u001B[34m(self, X, y, **fit_params)\u001B[39m\n\u001B[32m    903\u001B[39m         warnings.warn(\n\u001B[32m    904\u001B[39m             (\n\u001B[32m    905\u001B[39m                 \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mThis object (\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m.\u001B[34m__class__\u001B[39m.\u001B[34m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m) has a `transform`\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   (...)\u001B[39m\u001B[32m    913\u001B[39m             \u001B[38;5;167;01mUserWarning\u001B[39;00m,\n\u001B[32m    914\u001B[39m         )\n\u001B[32m    916\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m y \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    917\u001B[39m     \u001B[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m918\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m.transform(X)\n\u001B[32m    919\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    920\u001B[39m     \u001B[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001B[39;00m\n\u001B[32m    921\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.fit(X, y, **fit_params).transform(X)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Analisis-Exploratorio/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:894\u001B[39m, in \u001B[36mStandardScaler.fit\u001B[39m\u001B[34m(self, X, y, sample_weight)\u001B[39m\n\u001B[32m    892\u001B[39m \u001B[38;5;66;03m# Reset internal state before fitting\u001B[39;00m\n\u001B[32m    893\u001B[39m \u001B[38;5;28mself\u001B[39m._reset()\n\u001B[32m--> \u001B[39m\u001B[32m894\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mpartial_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Analisis-Exploratorio/.venv/lib/python3.12/site-packages/sklearn/base.py:1389\u001B[39m, in \u001B[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[39m\u001B[34m(estimator, *args, **kwargs)\u001B[39m\n\u001B[32m   1382\u001B[39m     estimator._validate_params()\n\u001B[32m   1384\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[32m   1385\u001B[39m     skip_parameter_validation=(\n\u001B[32m   1386\u001B[39m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[32m   1387\u001B[39m     )\n\u001B[32m   1388\u001B[39m ):\n\u001B[32m-> \u001B[39m\u001B[32m1389\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Analisis-Exploratorio/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:930\u001B[39m, in \u001B[36mStandardScaler.partial_fit\u001B[39m\u001B[34m(self, X, y, sample_weight)\u001B[39m\n\u001B[32m    898\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Online computation of mean and std on X for later scaling.\u001B[39;00m\n\u001B[32m    899\u001B[39m \n\u001B[32m    900\u001B[39m \u001B[33;03mAll of X is processed as a single batch. This is intended for cases\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    927\u001B[39m \u001B[33;03m    Fitted scaler.\u001B[39;00m\n\u001B[32m    928\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m    929\u001B[39m first_call = \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[33m\"\u001B[39m\u001B[33mn_samples_seen_\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m--> \u001B[39m\u001B[32m930\u001B[39m X = \u001B[43mvalidate_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    931\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    932\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    933\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[43m=\u001B[49m\u001B[43m(\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcsr\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mcsc\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    934\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mFLOAT_DTYPES\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    935\u001B[39m \u001B[43m    \u001B[49m\u001B[43mensure_all_finite\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mallow-nan\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    936\u001B[39m \u001B[43m    \u001B[49m\u001B[43mreset\u001B[49m\u001B[43m=\u001B[49m\u001B[43mfirst_call\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    937\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    938\u001B[39m n_features = X.shape[\u001B[32m1\u001B[39m]\n\u001B[32m    940\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m sample_weight \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Analisis-Exploratorio/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:2944\u001B[39m, in \u001B[36mvalidate_data\u001B[39m\u001B[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001B[39m\n\u001B[32m   2942\u001B[39m         out = X, y\n\u001B[32m   2943\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m no_val_y:\n\u001B[32m-> \u001B[39m\u001B[32m2944\u001B[39m     out = \u001B[43mcheck_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_name\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mX\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mcheck_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2945\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_y:\n\u001B[32m   2946\u001B[39m     out = _check_y(y, **check_params)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Analisis-Exploratorio/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:1107\u001B[39m, in \u001B[36mcheck_array\u001B[39m\u001B[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[39m\n\u001B[32m   1101\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1102\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mFound array with dim \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[33m. \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[33m expected <= 2.\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   1103\u001B[39m         % (array.ndim, estimator_name)\n\u001B[32m   1104\u001B[39m     )\n\u001B[32m   1106\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m ensure_all_finite:\n\u001B[32m-> \u001B[39m\u001B[32m1107\u001B[39m     \u001B[43m_assert_all_finite\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m   1108\u001B[39m \u001B[43m        \u001B[49m\u001B[43marray\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1109\u001B[39m \u001B[43m        \u001B[49m\u001B[43minput_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1110\u001B[39m \u001B[43m        \u001B[49m\u001B[43mestimator_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43mestimator_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m   1111\u001B[39m \u001B[43m        \u001B[49m\u001B[43mallow_nan\u001B[49m\u001B[43m=\u001B[49m\u001B[43mensure_all_finite\u001B[49m\u001B[43m \u001B[49m\u001B[43m==\u001B[49m\u001B[43m \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mallow-nan\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m   1112\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1114\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m copy:\n\u001B[32m   1115\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m _is_numpy_namespace(xp):\n\u001B[32m   1116\u001B[39m         \u001B[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Analisis-Exploratorio/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:120\u001B[39m, in \u001B[36m_assert_all_finite\u001B[39m\u001B[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001B[39m\n\u001B[32m    117\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m first_pass_isfinite:\n\u001B[32m    118\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m120\u001B[39m \u001B[43m_assert_all_finite_element_wise\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    121\u001B[39m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    122\u001B[39m \u001B[43m    \u001B[49m\u001B[43mxp\u001B[49m\u001B[43m=\u001B[49m\u001B[43mxp\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    123\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_nan\u001B[49m\u001B[43m=\u001B[49m\u001B[43mallow_nan\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    124\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmsg_dtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmsg_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    125\u001B[39m \u001B[43m    \u001B[49m\u001B[43mestimator_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43mestimator_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    126\u001B[39m \u001B[43m    \u001B[49m\u001B[43minput_name\u001B[49m\u001B[43m=\u001B[49m\u001B[43minput_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    127\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/PycharmProjects/Analisis-Exploratorio/.venv/lib/python3.12/site-packages/sklearn/utils/validation.py:169\u001B[39m, in \u001B[36m_assert_all_finite_element_wise\u001B[39m\u001B[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001B[39m\n\u001B[32m    152\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m estimator_name \u001B[38;5;129;01mand\u001B[39;00m input_name == \u001B[33m\"\u001B[39m\u001B[33mX\u001B[39m\u001B[33m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m has_nan_error:\n\u001B[32m    153\u001B[39m     \u001B[38;5;66;03m# Improve the error message on how to handle missing values in\u001B[39;00m\n\u001B[32m    154\u001B[39m     \u001B[38;5;66;03m# scikit-learn.\u001B[39;00m\n\u001B[32m    155\u001B[39m     msg_err += (\n\u001B[32m    156\u001B[39m         \u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mestimator_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m does not accept missing values\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    157\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m encoded as NaN natively. For supervised learning, you might want\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m   (...)\u001B[39m\u001B[32m    167\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33m#estimators-that-handle-nan-values\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    168\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m169\u001B[39m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg_err)\n",
      "\u001B[31mValueError\u001B[39m: Input X contains infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- 10. ARQUITECTURA DEL MODELO LSTM --------------------------------------\n",
    "def create_lstm_model(input_shape, units=64, dropout_rate=0.2):\n",
    "    \"\"\"\n",
    "    Crea un modelo LSTM para predicción de series temporales.\n",
    "\n",
    "    Args:\n",
    "        input_shape: Forma de los datos de entrada (time steps, features)\n",
    "        units: Número de unidades en la capa LSTM\n",
    "        dropout_rate: Tasa de dropout para regularización\n",
    "\n",
    "    Returns:\n",
    "        Modelo compilado\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        # Primera capa LSTM con retorno de secuencias para apilar capas LSTM\n",
    "        LSTM(units, return_sequences=True, input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "\n",
    "        # Segunda capa LSTM\n",
    "        LSTM(units // 2, return_sequences=False),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate),\n",
    "\n",
    "        # Capas densas para la salida\n",
    "        Dense(units // 4, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(dropout_rate / 2),\n",
    "\n",
    "        # Capa de salida (una sola predicción)\n",
    "        Dense(1)\n",
    "    ])\n",
    "\n",
    "    # Compilar modelo\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='mse',\n",
    "        metrics=['mae']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Crear modelo\n",
    "input_shape = (X_train_scaled.shape[1], X_train_scaled.shape[2])\n",
    "model = create_lstm_model(input_shape)\n",
    "model.summary()\n"
   ],
   "id": "e76e88199829318f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- 11. ENTRENAMIENTO DEL MODELO --------------------------------------\n",
    "# Callbacks para mejorar el entrenamiento\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=0.0001,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        '../models/lstm_weekly_best.h5',\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Entrenar modelo\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train_scaled,\n",
    "    validation_data=(X_val_scaled, y_val_scaled),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Guardar el modelo final\n",
    "model.save('../models/lstm_weekly_final.h5')\n",
    "print(\"Modelo guardado en '../models/lstm_weekly_final.h5'\")\n"
   ],
   "id": "8ca547a846e26d55"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- 12. VISUALIZACIÓN DEL ENTRENAMIENTO --------------------------------------\n",
    "# Graficar pérdida durante el entrenamiento\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Pérdida durante el entrenamiento')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('Pérdida (MSE)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['mae'], label='Train MAE')\n",
    "plt.plot(history.history['val_mae'], label='Validation MAE')\n",
    "plt.title('MAE durante el entrenamiento')\n",
    "plt.xlabel('Época')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "4cf80f89097f267f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- 13. EVALUACIÓN DEL MODELO --------------------------------------\n",
    "# Predecir en conjunto de prueba\n",
    "y_pred_scaled = model.predict(X_test_scaled)\n",
    "y_pred = y_scaler.inverse_transform(y_pred_scaled).flatten()\n",
    "\n",
    "# Calcular métricas\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "mape = mean_absolute_percentage_error(y_test, y_pred)\n",
    "\n",
    "print(\"\\n=== EVALUACIÓN DEL MODELO LSTM ===\")\n",
    "print(f\"Test MAE: {mae:.2f}\")\n",
    "print(f\"Test RMSE: {rmse:.2f}\")\n",
    "print(f\"Test R²: {r2:.4f}\")\n",
    "print(f\"Test MAPE: {mape:.4f}\")\n",
    "\n",
    "# Comparar con un modelo baseline (predicción con la media)\n",
    "y_pred_baseline = np.full_like(y_test, y_test.mean())\n",
    "r2_baseline = 0  # Por definición, R² de predecir la media es 0\n",
    "mae_baseline = mean_absolute_error(y_test, y_pred_baseline)\n",
    "\n",
    "print(f\"\\nMejora sobre baseline:\")\n",
    "print(f\"MAE Reduction: {(mae_baseline - mae) / mae_baseline:.2%}\")\n",
    "print(f\"R² Improvement: {r2 - r2_baseline:.4f}\")\n"
   ],
   "id": "290a3247c65d270c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- 14. VISUALIZACIÓN DE RESULTADOS --------------------------------------\n",
    "# Visualizar predicciones vs valores reales\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Predicciones vs valores reales\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel('Valores Reales')\n",
    "plt.ylabel('Predicciones')\n",
    "plt.title('Predicciones vs Valores Reales')\n",
    "\n",
    "# Histograma de errores\n",
    "plt.subplot(2, 2, 2)\n",
    "errors = y_pred - y_test\n",
    "plt.hist(errors, bins=50)\n",
    "plt.xlabel('Error de Predicción')\n",
    "plt.ylabel('Frecuencia')\n",
    "plt.title('Distribución de Errores')\n",
    "\n",
    "# Errores vs valores reales\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.scatter(y_test, errors, alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='-')\n",
    "plt.xlabel('Valores Reales')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Errores vs Valores Reales')\n",
    "\n",
    "# Predicciones a lo largo del tiempo para un cliente aleatorio\n",
    "plt.subplot(2, 2, 4)\n",
    "# Seleccionar un cliente aleatorio del conjunto de prueba\n",
    "random_client = np.random.choice(test_clients)\n",
    "client_indices = [i for i, client_id in enumerate(test_client_map) if client_id == random_client]\n",
    "\n",
    "if client_indices:\n",
    "    client_y_test = y_test[client_indices]\n",
    "    client_y_pred = y_pred[client_indices]\n",
    "    plt.plot(range(len(client_y_test)), client_y_test, 'b-', label='Real')\n",
    "    plt.plot(range(len(client_y_pred)), client_y_pred, 'r-', label='Predicción')\n",
    "    plt.xlabel('Semana')\n",
    "    plt.ylabel('Gasto')\n",
    "    plt.title(f'Predicciones para Cliente ID: {random_client}')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Análisis por segmentos\n",
    "print(\"\\nAnálisis por segmentos:\")\n",
    "# Crear segmentos basados en el gasto real\n",
    "y_test_df = pd.DataFrame({'spend_next': y_test, 'prediction': y_pred})\n",
    "y_test_df['spend_segment'] = pd.qcut(y_test_df['spend_next'], 4, labels=['Bajo', 'Medio-Bajo', 'Medio-Alto', 'Alto'])\n",
    "\n",
    "# Calcular métricas por segmento\n",
    "segment_metrics = y_test_df.groupby('spend_segment').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'MAE': mean_absolute_error(x['spend_next'], x['prediction']),\n",
    "        'RMSE': np.sqrt(mean_squared_error(x['spend_next'], x['prediction'])),\n",
    "        'R²': r2_score(x['spend_next'], x['prediction']) if len(x) > 1 else np.nan,\n",
    "        'Count': len(x)\n",
    "    })\n",
    ")\n",
    "print(segment_metrics)\n"
   ],
   "id": "44661c570536dea7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- 15. FUNCIONES DE PREDICCIÓN -------------------------------------------\n",
    "def prepare_client_sequence(client_id, panel_data, feature_columns, seq_length=6):\n",
    "    \"\"\"\n",
    "    Prepara la secuencia de datos más reciente para un cliente específico.\n",
    "\n",
    "    Args:\n",
    "        client_id: ID del cliente\n",
    "        panel_data: DataFrame con todos los datos\n",
    "        feature_columns: Lista de columnas de características\n",
    "        seq_length: Longitud de la secuencia\n",
    "\n",
    "    Returns:\n",
    "        Secuencia de datos preparada para el modelo LSTM\n",
    "    \"\"\"\n",
    "    # Filtrar datos del cliente y ordenar por semana\n",
    "    client_data = panel_data[panel_data['id'] == client_id].sort_values('week')\n",
    "\n",
    "    if len(client_data) < seq_length:\n",
    "        return None\n",
    "\n",
    "    # Tomar las últimas 'seq_length' semanas\n",
    "    recent_data = client_data.tail(seq_length)\n",
    "\n",
    "    # Extraer características\n",
    "    features = recent_data[feature_columns].values\n",
    "\n",
    "    # Reshape para el modelo LSTM\n",
    "    sequence = features.reshape(1, seq_length, len(feature_columns))\n",
    "\n",
    "    return sequence\n",
    "\n",
    "def predict_client_next_week(model, client_id, panel_data, feature_columns, scaler, y_scaler, seq_length=6):\n",
    "    \"\"\"\n",
    "    Predice el gasto de la próxima semana para un cliente específico.\n",
    "\n",
    "    Args:\n",
    "        model: Modelo LSTM entrenado\n",
    "        client_id: ID del cliente\n",
    "        panel_data: DataFrame con todos los datos\n",
    "        feature_columns: Lista de columnas de características\n",
    "        scaler: Scaler para normalizar características\n",
    "        y_scaler: Scaler para desnormalizar la predicción\n",
    "        seq_length: Longitud de la secuencia\n",
    "\n",
    "    Returns:\n",
    "        Predicción del gasto para la próxima semana\n",
    "    \"\"\"\n",
    "    # Preparar secuencia\n",
    "    sequence = prepare_client_sequence(client_id, panel_data, feature_columns, seq_length)\n",
    "\n",
    "    if sequence is None:\n",
    "        return None\n",
    "\n",
    "    # Normalizar secuencia\n",
    "    sequence_reshaped = sequence.reshape(seq_length, len(feature_columns))\n",
    "    sequence_scaled = scaler.transform(sequence_reshaped)\n",
    "    sequence_scaled = sequence_scaled.reshape(1, seq_length, len(feature_columns))\n",
    "\n",
    "    # Predecir\n",
    "    prediction_scaled = model.predict(sequence_scaled)\n",
    "\n",
    "    # Desnormalizar predicción\n",
    "    prediction = y_scaler.inverse_transform(prediction_scaled)[0][0]\n",
    "\n",
    "    return float(prediction)\n",
    "\n",
    "# Calcular medianas por segmento para clientes con pocos datos\n",
    "segment_medians = panel.groupby(\"id_estado\")[\"spend_next\"].median().to_dict()\n",
    "\n",
    "def safe_predict(client_id):\n",
    "    \"\"\"\n",
    "    Función de predicción segura que maneja clientes con historial limitado.\n",
    "\n",
    "    Args:\n",
    "        client_id: ID del cliente\n",
    "\n",
    "    Returns:\n",
    "        Predicción del gasto para la próxima semana\n",
    "    \"\"\"\n",
    "    hist_len = panel[panel[\"id\"] == client_id][\"spend\"].count()\n",
    "\n",
    "    # Para datos semanales, requerimos al menos 6 semanas de historial\n",
    "    if hist_len < 6:\n",
    "        # Si no hay suficiente historial, usar la mediana del segmento\n",
    "        segment = panel.loc[panel[\"id\"] == client_id, \"id_estado\"].iat[0]\n",
    "        return segment_medians.get(segment, 0)\n",
    "\n",
    "    # Predecir con el modelo LSTM\n",
    "    prediction = predict_client_next_week(\n",
    "        model, client_id, panel_encoded, feature_cols, \n",
    "        scaler, y_scaler, seq_length=6\n",
    "    )\n",
    "\n",
    "    # Si la predicción falla, usar la mediana del segmento\n",
    "    if prediction is None:\n",
    "        segment = panel.loc[panel[\"id\"] == client_id, \"id_estado\"].iat[0]\n",
    "        return segment_medians.get(segment, 0)\n",
    "\n",
    "    return prediction\n"
   ],
   "id": "d003c5fd260ad395"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- 16. SERVIDOR FASTAPI ----------------------------------------------\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/forecast/{client_id}\")\n",
    "def forecast(client_id: int):\n",
    "    \"\"\"Endpoint para predecir el gasto de la próxima semana de un cliente\"\"\"\n",
    "    y_hat = safe_predict(client_id)\n",
    "    return {\"client_id\": client_id, \"next_week_spend\": y_hat}\n"
   ],
   "id": "c62936bd2712749"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- 17. GUARDAR RESULTADOS DE EVALUACIÓN --------------------------------------\n",
    "# Guardar resultados de evaluación\n",
    "evaluation_results = {\n",
    "    'MAE': float(mae),\n",
    "    'RMSE': float(rmse),\n",
    "    'R²': float(r2),\n",
    "    'MAPE': float(mape),\n",
    "    'Segment_Metrics': segment_metrics.to_dict()\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('../models/lstm_weekly_evaluation.json', 'w') as f:\n",
    "    json.dump({k: v for k, v in evaluation_results.items() if not isinstance(v, pd.DataFrame)}, f, indent=4)\n",
    "print(\"\\nResultados de evaluación guardados en '../models/lstm_weekly_evaluation.json'\")\n"
   ],
   "id": "cc46c6378b60a9be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- 18. COMPARACIÓN CON MODELO LGBM --------------------------------------\n",
    "try:\n",
    "    # Intentar cargar resultados de evaluación del modelo LightGBM\n",
    "    with open('../models/weekly_model_evaluation.json', 'r') as f:\n",
    "        lgbm_results = json.load(f)\n",
    "\n",
    "    # Crear tabla comparativa\n",
    "    comparison = pd.DataFrame({\n",
    "        'Métrica': ['MAE', 'RMSE', 'R²', 'MAPE'],\n",
    "        'LSTM': [mae, rmse, r2, mape],\n",
    "        'LightGBM': [\n",
    "            lgbm_results.get('MAE', 'N/A'),\n",
    "            lgbm_results.get('RMSE', 'N/A'),\n",
    "            lgbm_results.get('R²', 'N/A'),\n",
    "            lgbm_results.get('MAPE', 'N/A')\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    print(\"\\n=== COMPARACIÓN DE MODELOS ===\")\n",
    "    print(comparison)\n",
    "\n",
    "    # Visualizar comparación\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    metrics = ['MAE', 'RMSE', 'MAPE']\n",
    "    x = np.arange(len(metrics))\n",
    "    width = 0.35\n",
    "\n",
    "    lstm_values = [mae, rmse, mape]\n",
    "    lgbm_values = [\n",
    "        lgbm_results.get('MAE', 0),\n",
    "        lgbm_results.get('RMSE', 0),\n",
    "        lgbm_results.get('MAPE', 0)\n",
    "    ]\n",
    "\n",
    "    plt.bar(x - width/2, lstm_values, width, label='LSTM')\n",
    "    plt.bar(x + width/2, lgbm_values, width, label='LightGBM')\n",
    "\n",
    "    plt.xlabel('Métrica')\n",
    "    plt.ylabel('Valor')\n",
    "    plt.title('Comparación de Modelos')\n",
    "    plt.xticks(x, metrics)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Comparar R² en un gráfico separado\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    r2_values = [r2, lgbm_results.get('R²', 0)]\n",
    "    plt.bar(['LSTM', 'LightGBM'], r2_values)\n",
    "    plt.xlabel('Modelo')\n",
    "    plt.ylabel('R²')\n",
    "    plt.title('Comparación de R² entre Modelos')\n",
    "    plt.ylim(0, max(r2_values) * 1.2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"No se pudo cargar los resultados del modelo LightGBM: {e}\")\n"
   ],
   "id": "ec903223a371146f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# --- 19. CONCLUSIONES --------------------------------------\n",
    "\"\"\"\n",
    "# Conclusiones del Modelo LSTM\n",
    "\n",
    "## Rendimiento del Modelo\n",
    "- El modelo LSTM ha sido entrenado para predecir el gasto semanal de los clientes.\n",
    "- Se utilizaron secuencias de 6 semanas para capturar patrones temporales.\n",
    "- Las métricas de evaluación muestran un rendimiento [comparar con LightGBM según resultados].\n",
    "\n",
    "## Ventajas del Enfoque LSTM\n",
    "- Capacidad para capturar dependencias temporales a largo plazo.\n",
    "- Manejo efectivo de la estacionalidad y tendencias en los datos.\n",
    "- Robustez ante valores atípicos gracias a la normalización y arquitectura del modelo.\n",
    "\n",
    "## Limitaciones\n",
    "- Requiere suficiente historial (al menos 6 semanas) para hacer predicciones precisas.\n",
    "- Mayor complejidad computacional comparado con modelos más simples.\n",
    "- Sensibilidad a la calidad y completitud de los datos históricos.\n",
    "\n",
    "## Recomendaciones\n",
    "- Considerar un enfoque de ensamblaje combinando LSTM con LightGBM para mejorar la precisión.\n",
    "- Explorar arquitecturas más complejas como redes bidireccionales o atención para mejorar el rendimiento.\n",
    "- Implementar un sistema de monitoreo para detectar degradación del modelo con el tiempo.\n",
    "\"\"\"\n"
   ],
   "id": "eae180c098ecf988"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
